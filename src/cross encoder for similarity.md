ğŸ¤– Best Cross-Encoder for Sentence Pair Classification
This project focuses on detecting the relationship between sentence pairs using a fine-tuned Cross-Encoder model. The architecture takes both sentences as a single input and directly predicts a similarity label â€” achieving high accuracy for tasks like paraphrase detection and semantic matching.
We use a state-of-the-art transformer-based DeBERTa-v3-Large model fine-tuned on a custom binary classification dataset.

ğŸ§  Model Used
nli-deberta-v3-large (Cross-Encoder)
Source: cross-encoder/nli-deberta-v3-large from Hugging Face
Task: Binary Classification
0 = Not Duplicate
1 = Duplicate / Semantically Similar
Fine-tuned on a labeled dataset with sentence pairs (question1, question2)
Optimized using:
AdamW optimizer
Linear warm-up scheduler
CrossEntropyLoss

ğŸ“Š Results
âœ… Validation Accuracy: ~88â€“90%
ğŸ“‰ Loss: Rapid convergence after few epochs
ğŸ§ª Evaluated using:
Accuracy
Precision, Recall, F1-score
Confusion matrix

ğŸ” How It Works
Cross-Encoder inputs both sentences together:
"[CLS] Sentence1 [SEP] Sentence2 [SEP]"
Unlike bi-encoders, it does not embed sentences separately â€” leading to much better accuracy on fine-grained classification tasks.

ğŸ“ˆ Why Cross-Encoder?
âœ… Learns richer interaction between sentence pairs
âš ï¸ Slower inference than bi-encoder (not ideal for real-time bulk retrieval)
ğŸ¯ Ideal for high-accuracy tasks like duplicate detection, NLI, QA reranking

ğŸ§ª Future Improvements
Add early stopping and model checkpointing
Hyperparameter tuning (batch size, learning rate)
Export to ONNX for deployment

ğŸ“ License
This project is open for research and educational purposes. You may adapt it for your academic or personal use.



